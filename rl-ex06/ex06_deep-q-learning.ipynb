{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccbf74e-1fcb-40c2-98d7-c30d0f73f252",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "In this exercise we will build a simple deep Q-learning agent from scratch. For this we need to look at the following tasks:\n",
    "* How do we specify the model?\n",
    "* How do we calculate an action?\n",
    "* How do we sample episodes?\n",
    "* How do we train the model?\n",
    "\n",
    "We will develop this by implementing a class for the agent. We will use a package called jdc, that will allow us to split the implementation of a class over several cells.\n",
    "\n",
    "We will be using torch for the implementation of the neural network and the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18836b84-bfcc-4d2e-a25c-cd1d9dbef9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /opt/conda/lib/python3.12/site-packages (0.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "import jdc\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d78c58-adcf-4645-9916-1a3316121f12",
   "metadata": {},
   "source": [
    "## Example: Cart Pole\n",
    "We will use an environment from OpenAI for this exercise. The goal is to balance a pole by moving the attached cart to the left or to the right. As the pole should be balanced as long as possible, the reward for each time step is +1. An episode is done when the angle of the pole becomes too large.\n",
    "\n",
    "The observation space gives some measurements about the pole, for example the angle. However, we actually do not need to know the specific details, as the neural network will just learn using the input.\n",
    "\n",
    "The actions are to move the cart to the left or to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6fccfa-3da9-4939-a71e-f1ae82ceb64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n",
      "Sample from the observation space: [ 4.21415     0.5104905  -0.40308246 -0.5030688 ]\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')\n",
    "\n",
    "print(f'Sample from the observation space: {env.observation_space.sample()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6bac-94a9-42ec-ad59-9a5bade325e3",
   "metadata": {},
   "source": [
    "The environment has a render function that we can use to display the state. The parameter 'render_mode' is used to specify the mode. For standalone applications, we can use 'human' that will open a window. Here we get the image as an array and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20961d56-4fe5-45d4-b67b-b791cb864706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc811ef-a89b-4dd0-b385-c199d8a535d5",
   "metadata": {},
   "source": [
    "We can also try to render a sequence of images. Depending on your browser the display might not be optimal...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780d4e76-70ea-45b4-9b90-205cf6dbeb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with a few steps\n",
    "env.reset()\n",
    "for _ in range(50):\n",
    "    env.step(env.action_space.sample())  # Take a random action\n",
    "    display_environment(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72df12-f213-4217-84fe-4b18b9bbdb94",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Our agent will use the class of the neural network (the model) as parameter, so that we can call it using different models. The first task is to write a example class that for the neural network. We will pass the number of observation values and the number of actions values as parameters.\n",
    "\n",
    "The model must calculate the q function for each state. We will also need to access those values for evaluation.\n",
    "\n",
    "Build a sequential model using at least two dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ab168b-042e-403a-8e32-e5d2e98e721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # test if it worked\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using CUDA device')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using MPS device')\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc9c401-f28d-4ed5-bfde-5990b8f3de02",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4f4206b89ef19b7d4ab280852a1f0bc",
     "grade": false,
     "grade_id": "cell-5f57f2a0435bd064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_action):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        # generate a sequential model in a internal variable (for example self.fc)\n",
    "        # YOUR CODE HERE\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_obs, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_action)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_tensor):\n",
    "        # forward should just call you model\n",
    "        # YOUR CODE HERE\n",
    "        return self.fc(x_tensor)\n",
    "\n",
    "    def q_values(self, obs_tensor):\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(obs_tensor)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9787f0e3-966e-4d85-94f8-e3950d45f90a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9879b8c8b060c8ca5b8a646a2825b5",
     "grade": true,
     "grade_id": "cell-02f08792391930df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNetwork(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = DQNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "print(m)\n",
    "\n",
    "\n",
    "obs_sample = env.observation_space.sample()\n",
    "\n",
    "# models expect a batch of data, so we have to add a dimension and convert it to a tensor\n",
    "obs_batch = np.expand_dims(obs_sample, axis=0)\n",
    "obs_batch_tensor = torch.from_numpy(obs_batch).float()\n",
    "action_values = m.q_values(obs_batch_tensor)\n",
    "assert action_values.shape == (1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acfd08-b0fe-41e2-9ad3-47ef889cfa55",
   "metadata": {},
   "source": [
    "### Device managment in torch\n",
    "\n",
    "In order to use the gpu, we have to move our models to the device. We will also set this as\n",
    "default device, so that any tensors will be initialized on it and we don't have to specify them every time.\n",
    "\n",
    "In the code, we will use _tensor whenever a torch tensor is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3cfd11-13eb-4fd3-a850-e3a0262446aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cd294-1491-4191-a25b-de8aafcd7762",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. Check the parameters and their descriptions as they will be used in the implementation and you will have to find suitable hyperparameters for them.\n",
    "\n",
    "\n",
    "The only thing to fill out is the optimizer and the loss function. \n",
    "\n",
    "There are different optimizers available, either standand SGD or Adam would be possible and should be initialized with the learning rate given in the parameters.\n",
    "\n",
    "What is the loss function that we have to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11fc0213-5dc9-4f23-ab2d-1d44d983a144",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc682665d090e45871c7275589f8d3fd",
     "grade": false,
     "grade_id": "cell-8c19c66d131c3fbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space,\n",
    "                 model_cls,\n",
    "                 device,\n",
    "                 gamma: float,\n",
    "                 epsilon: float, epsilon_decay: float, epsilon_min: float,\n",
    "                 learning_rate: float, training_frequency: int, target_update_frequency: int,\n",
    "                 tau : float, use_double_dqn: bool,\n",
    "                 batch_size: int, memory_size: int):\n",
    "        \"\"\"\n",
    "        Initialise the agent.\n",
    "        Args:\n",
    "            observation_space: The observation space of the environment\n",
    "            action_space: The action space of the environment\n",
    "            model_cls: the class that implements the model,\n",
    "            device: the device to run torch on\n",
    "            gamma: The discount factor\n",
    "            epsilon: The initial epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: The decay factor for the epsilon value\n",
    "            epsilon_min: The minimal epsilon value after which it will not be decayed further\n",
    "            learning_rate: The learning rate for the optimizer\n",
    "            training_frequency: The frequency (in steps) of training the model\n",
    "            target_update_frequency: The frequency (in steps) of updating the target model (if not using tau)\n",
    "            tau: weight of the new model in the target update \n",
    "            use_double_dqn: use double q learning\n",
    "            batch_size: The batch size for training (sampled from the memory)\n",
    "            memory_size: The size of the memory for storing experiences\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "    \n",
    "        # hyperparameters from parameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.training_frequency = training_frequency\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "    \n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.tau = tau\n",
    "\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "        # internal variables\n",
    "        self.nr_steps = 0\n",
    "    \n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "\n",
    "        self.device = device\n",
    "    \n",
    "        # build the models\n",
    "        # self.model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        # self.target_model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        self.model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "        self.target_model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model = self.target_model.to(self.device)\n",
    "\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.loss = None \n",
    "\n",
    "        # generate the optimizer and loss function in the variables above\n",
    "        # YOUR CODE HERE\n",
    "        self.optimizer = optim.Adam(self.model.parameters() ,lr=learning_rate)\n",
    "        self.loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent to the initial state\n",
    "        \"\"\"\n",
    "        self.nr_steps = 0\n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "        self.nr_training_steps = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf988c-24c5-4b8e-ad91-e79fec78d7b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee19ca899da78a8af841c665edc8caf",
     "grade": true,
     "grade_id": "cell-6638146a1a48950b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe8a89-c401-40bc-a44e-4052b3a3d396",
   "metadata": {},
   "source": [
    "### Updating of the target model\n",
    "\n",
    "It is important in Q-Learning that the model that is trained and the target model used to calculate the target functions are not the same. The target model should stay fixed for a while or change only slowly.\n",
    "\n",
    "There are different methods to update the model:\n",
    "- Replace the target model every number of steps\n",
    "- Interpolate between the policy and the target model\n",
    "\n",
    "Implement the update to use the replacement when the parameter tau is equal to 0 and the interpolation (using tau) when tau is greater than 0. You should later experiment with both update functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bcc187-fd5a-4689-ba07-f7e62d128f13",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e02fcb0302750b798ad2be8e7a21010",
     "grade": false,
     "grade_id": "cell-a1cb418924b7cb27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Update the target model with the weights from the current model. There are two possibilities for\n",
    "        updating:\n",
    "        - Replace the target model every number of steps\n",
    "        - Interpolate between the policy and the target model\n",
    "\n",
    "        In both cases the update_target_model method will be called every self.target_update_frequency \n",
    "        number of steps (not training steps). If the update is done by replacement, the value should be\n",
    "        higher, if the update is done by interpolation it can be lower.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        policy_net_state_dict = self.model.state_dict()\n",
    "        target_model_state_dict = self.target_model.state_dict()\n",
    "\n",
    "        if self.tau > 0.0:\n",
    "            # YOUR CODE HERE\n",
    "            for key in policy_net_state_dict:\n",
    "                target_model_state_dict[key] = policy_net_state_dict[key] * self.tau + target_model_state_dict[key] *(1-self.tau)\n",
    "            self.target_model.load_state_dict(target_model_state_dict)\n",
    "        else:\n",
    "            # YOUR CODE HERE\n",
    "            self.target_model.load_state_dict(policy_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbda48da-2108-4a2e-83a8-957b81d59023",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbcce6aa140ca971b00622c3915af067",
     "grade": true,
     "grade_id": "cell-cb441f633638dff8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n",
    "q_agent.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b09c6-c2b2-45dc-81e2-c87179dab829",
   "metadata": {},
   "source": [
    "## Calculating actions\n",
    "\n",
    "Calculating actions is done with an epsilon greedy policy. However, for evaluation it is often suitable to use the greedy policy instead. So we add a parameter `stochastic`, if it is True then the epsilon-greedy policy is used, if not, the greedy policy is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808c4b3c-82ec-456a-beb4-f94a3bc1e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def calculate_action(self, obs_tensor, stochastic: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the action for the given observation.\n",
    "    Args:\n",
    "        obs: the observation\n",
    "        stochastic: whether to use a stochastic (epsilon greedy) policy or not\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    if not stochastic or np.random.rand() > self.epsilon:\n",
    "        # calculate greedy action\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model.q_values(obs_tensor)\n",
    "        return torch.argmax(action_value).numpy(force=True)\n",
    "    else:\n",
    "        # calculate random action\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d490aa0b-73f6-4001-8ee5-0387ff91a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "obs_tensor = torch.tensor(env.observation_space.sample())\n",
    "a = agent.calculate_action(obs_tensor)\n",
    "assert env.action_space.contains(a)\n",
    "a = agent.calculate_action(obs_tensor, stochastic=False)\n",
    "assert env.action_space.contains(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cbec0-6b65-40b4-9bbf-c0c136afd366",
   "metadata": {},
   "source": [
    "## Add the steps\n",
    "\n",
    "Next we will add the two step functions as in the previous implementations of an agent. The `step_first` method is called after the environment is reset and we do not have any rewards yet.\n",
    "\n",
    "The `step` method is called for all other steps. In the step method we need to\n",
    "* Save the current experience (S, A, R, S', done) in the memory.\n",
    "* Calculate the next action\n",
    "* Save action and observation for next step\n",
    "* train the model every couple of steps\n",
    "* update the target model every couple of steps\n",
    "\n",
    "You have to fill in the code for the first three items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922fa8c8-bc5d-4552-8709-91c6a8c9c421",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edc27e1ff54fbb99d45bdecba8e633e3",
     "grade": false,
     "grade_id": "cell-f11d5c8957197292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent    \n",
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    self.last_obs = np.array(obs)\n",
    "    obs_tensor = torch.tensor(self.last_obs, dtype=torch.float32)\n",
    "    self.last_action = self.calculate_action(obs_tensor)\n",
    "    \n",
    "    return self.last_action\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    # Konvertiere die Beobachtung in ein NumPy-Array und dann in einen Tensor\n",
    "    obs = np.array(obs)  # Um sicherzustellen, dass es ein Array ist\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)  # Umwandlung in Tensor\n",
    "    \n",
    "    # Füge die letzte Erfahrung zum Replay-Speicher hinzu\n",
    "    self.memory.append((self.last_obs, self.last_action, reward, obs, done))\n",
    "\n",
    "    # Berechne die nächste Aktion\n",
    "    self.last_action = self.calculate_action(obs_tensor)\n",
    "    self.last_obs = obs  # Aktualisiere die letzte Beobachtung\n",
    "    self.nr_steps += 1\n",
    "\n",
    "    if self.nr_steps % self.training_frequency == 0:\n",
    "        self.train_model()\n",
    "\n",
    "    if self.nr_steps % self.target_update_frequency == 0:\n",
    "        self.update_target_model()\n",
    "\n",
    "    return self.last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679ad371-d595-4b86-9b6d-322f0d2c3434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68f17895cc3cd32aa4caf076cf644041",
     "grade": true,
     "grade_id": "cell-55874102f12eeb67",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "a = agent.step_first(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e1f9b-2b90-4d3f-94b7-adf575b31e33",
   "metadata": {},
   "source": [
    "## Train the model function\n",
    "\n",
    "The last thing to do is now to train the model. For this we have to \n",
    "* Sample from the memory to get a batch of observations, actions, rewards, next observations and dones\n",
    "* Calculate a better estimate for the q values of the current observation using q-learning\n",
    "* Fit the model to the updated values using gradient descend on the loss\n",
    "* Decay the epsilon value\n",
    "\n",
    "In torch, the gradient descend step has to be calculated in the code. Todo this:\n",
    "* Calculate the model output\n",
    "* Calculate the loss function\n",
    "* Clear the gradient (using self.optimizer.zero_grad())\n",
    "* Calculate a backwards step (resulting in the gradient)\n",
    "* Apply the step in the optimizer\n",
    "\n",
    "Note that between calculating the model, and calculating the loss function, you will have to calculate the target function (using the target_model), as no gradients are required on the target model, this code should be within a `with torch.no_grad()` block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a5506a-0c18-4433-8b1d-529177bc2cf4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642443bea50b66830d541a7458e57b1d",
     "grade": false,
     "grade_id": "cell-fe51b24157ad1aa1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train_model(self):\n",
    "    # not enough samples yet\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "\n",
    "    self.nr_training_steps += 1\n",
    "\n",
    "    # Sample random minibatch from memory\n",
    "    indices = np.random.choice(len(self.memory), size=self.batch_size, replace=False)\n",
    "\n",
    "    # Unpack batch\n",
    "    obs = np.array([self.memory[i][0] for i in indices])\n",
    "    actions = np.array([self.memory[i][1] for i in indices])\n",
    "    rewards = np.array([self.memory[i][2] for i in indices])\n",
    "    obs_next = np.array([self.memory[i][3] for i in indices])\n",
    "    dones = np.array([self.memory[i][4] for i in indices])\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "    obs_next_tensor = torch.tensor(obs_next, dtype=torch.float32).to(self.device)\n",
    "    dones_tensor = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "    # Calculate Q-values for the current observation using the model\n",
    "    q_values = self.model(obs_tensor).gather(dim=1, index=actions_tensor)\n",
    "\n",
    "    # Calculate the target Q-values using the target model (without gradients)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = self.target_model(obs_next_tensor).max(1, keepdim=True)[0]\n",
    "        target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * next_q_values\n",
    "\n",
    "    # Compute the loss between current Q-values and target Q-values\n",
    "    loss = self.loss(q_values, target_q_values)\n",
    "\n",
    "    # Backpropagation to compute gradients\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53e246cf-b487-4e93-acc9-1b18e3d1417e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b4f1b16af9ee8053a8839b775cab2e9",
     "grade": true,
     "grade_id": "cell-38fb10ebd1e39242",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test with a small version of the agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=20,\n",
    "                       tau=0.00,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=4,\n",
    "                       memory_size=16)\n",
    "obs, info = env.reset()\n",
    "action = agent.step_first(obs)\n",
    "\n",
    "for i in range(20):\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        action = agent.step_first(obs)\n",
    "assert agent.nr_training_steps > 4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b96b12-e8dd-41f6-a8dc-6acd8c2f8223",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented a full DQN Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ef19c-2e2a-4d97-a173-34322b805f55",
   "metadata": {},
   "source": [
    "## Complete agent for training and evaluation.\n",
    "\n",
    "We will add some additional methods to the agent in order to train and evaluate it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b25676-7364-48ef-9ebb-9ac55e730bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, env: gym.Env, nr_episodes_to_train: int,  eval_env: gym.Env, eval_frequency: int):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of steps.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_to_train: the number of episodes to train\n",
    "        eval_env: Environment for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            done = done or truncated\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, 10)\n",
    "            print(f'Evaluation: episode {nr_episodes}, epsilon: {self.epsilon} mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes >= nr_episodes_to_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "        a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        # some environments do not support truncated episodes, so we additionally check for a maximal number of steps\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "            a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e050db9-1f7d-4290-9e6d-03c4a936fa69",
   "metadata": {},
   "source": [
    "## Example Training\n",
    "\n",
    "Here is an example with some hyperparameters and a short training time (that will not be enough to actually train the full agent). You can adjust the parameters and see if you get good results. But for handing in the exercise, put it back to a short training :-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "732052e5-bc4f-40d4-a496-18babc377d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: episode 10, epsilon: 0.4483407572091659 mean reward: 46.6\n",
      "Evaluation: episode 20, epsilon: 0.3334208358186289 mean reward: 9.9\n",
      "Evaluation: episode 30, epsilon: 0.2989723000004739 mean reward: 9.4\n",
      "Evaluation: episode 40, epsilon: 0.2686199059255016 mean reward: 9.6\n",
      "Evaluation: episode 50, epsilon: 0.24231677478799676 mean reward: 9.5\n",
      "Evaluation: episode 60, epsilon: 0.21858923351697282 mean reward: 8.9\n",
      "Evaluation: episode 70, epsilon: 0.19561311110057522 mean reward: 9.7\n",
      "Evaluation: episode 80, epsilon: 0.1612639109059692 mean reward: 45.4\n",
      "Evaluation: episode 90, epsilon: 0.10188172361156822 mean reward: 60.6\n",
      "Evaluation: episode 100, epsilon: 0.043527254602211976 mean reward: 107.5\n",
      "Evaluation: episode 110, epsilon: 0.029970113248546433 mean reward: 382.7\n",
      "Evaluation: episode 120, epsilon: 0.029970113248546433 mean reward: 340.1\n",
      "Evaluation: episode 130, epsilon: 0.029970113248546433 mean reward: 432.1\n",
      "Evaluation: episode 140, epsilon: 0.029970113248546433 mean reward: 323.7\n",
      "Evaluation: episode 150, epsilon: 0.029970113248546433 mean reward: 261.3\n",
      "Evaluation: episode 160, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 170, epsilon: 0.029970113248546433 mean reward: 120.4\n",
      "Evaluation: episode 180, epsilon: 0.029970113248546433 mean reward: 32.5\n",
      "Evaluation: episode 190, epsilon: 0.029970113248546433 mean reward: 27.3\n",
      "Evaluation: episode 200, epsilon: 0.029970113248546433 mean reward: 30.3\n",
      "Evaluation: episode 210, epsilon: 0.029970113248546433 mean reward: 36.4\n",
      "Evaluation: episode 220, epsilon: 0.029970113248546433 mean reward: 325.3\n",
      "Evaluation: episode 230, epsilon: 0.029970113248546433 mean reward: 72.6\n",
      "Evaluation: episode 240, epsilon: 0.029970113248546433 mean reward: 108.1\n",
      "Evaluation: episode 250, epsilon: 0.029970113248546433 mean reward: 107.9\n",
      "Evaluation: episode 260, epsilon: 0.029970113248546433 mean reward: 127.3\n",
      "Evaluation: episode 270, epsilon: 0.029970113248546433 mean reward: 181.1\n",
      "Evaluation: episode 280, epsilon: 0.029970113248546433 mean reward: 206.0\n",
      "Evaluation: episode 290, epsilon: 0.029970113248546433 mean reward: 472.2\n",
      "Evaluation: episode 300, epsilon: 0.029970113248546433 mean reward: 494.2\n",
      "Evaluation: episode 310, epsilon: 0.029970113248546433 mean reward: 499.6\n",
      "Evaluation: episode 320, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 330, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 340, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 350, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 360, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 370, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 380, epsilon: 0.029970113248546433 mean reward: 500.0\n",
      "Evaluation: episode 390, epsilon: 0.029970113248546433 mean reward: 260.6\n",
      "Evaluation: episode 400, epsilon: 0.029970113248546433 mean reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "env_train = gym.make(environment_name)\n",
    "env_eval = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    " # Hyperparameters will be quite important here.\n",
    "q_agent = DQNAgent(env_train.observation_space, env_train.action_space,\n",
    "                   model_cls=DQNetwork,\n",
    "                   device=device,\n",
    "                   gamma=0.99,\n",
    "                   epsilon=0.5,\n",
    "                   epsilon_min=0.03,\n",
    "                   epsilon_decay=0.999,\n",
    "                   learning_rate=0.0005,\n",
    "                   training_frequency=1,\n",
    "                   target_update_frequency=2,\n",
    "                   tau=0.01,\n",
    "                   use_double_dqn=False,\n",
    "                   batch_size=256,\n",
    "                   memory_size=10000)\n",
    "\n",
    "# the parameters are not optimal, but should already give a result\n",
    "q_agent.train(env_train, nr_episodes_to_train=400, eval_env=env_eval, eval_frequency=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe8b3c3b-1d54-4e5a-8a07-cef082a51899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500.0, 500.0, 500.0, 500.0, 500.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_agent.evaluate(env_eval, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcb7ac82-ea78-4f37-bb3d-86cc843a1d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d4871-50fa-4122-97a5-134798d52b52",
   "metadata": {},
   "source": [
    "# Bonus Exercise\n",
    "\n",
    "Can you try training for another example, like the mountain car from the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f787162-82ef-4a51-8e8e-d488863dfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "other_env = 'MountainCar-v0'\n",
    "\n",
    "env_train = gym.make(other_env)\n",
    "env_eval = gym.make(other_env, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env_eval.observation_space}')\n",
    "print(f'Action space: {env_eval.action_space}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8aaaaa-c62e-4169-9dbd-8d8bcc9b0350",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4623ab4dec6a1df0eeb4f6c804f37f13",
     "grade": false,
     "grade_id": "cell-a4cde5d226be73e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df69d7ad-d39a-454b-baed-3afb680f8f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 4x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(obs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mq_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     obs, _, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Take a random action\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(obs)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m<string>:13\u001b[0m, in \u001b[0;36mcalculate_action\u001b[0;34m(self, obs_tensor, stochastic)\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mDQNetwork.q_values\u001b[0;34m(self, obs_tensor)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mq_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs_tensor):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mDQNetwork.forward\u001b[0;34m(self, x_tensor)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_tensor):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# forward should just call you model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 4x32)"
     ]
    }
   ],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(other_env, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e40e83-197e-4c0d-8915-77e241053ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
