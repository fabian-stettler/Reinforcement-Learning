{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af228dc-9528-469e-9b29-36f26b339430",
   "metadata": {},
   "source": [
    "# REINFORCE: Vanilla Policy Gradient\n",
    "\n",
    "In this exercise we want to implement the REINFORCE policy-gradient method to solve a reinforcement learning problem.\n",
    "\n",
    "In order to develop the algorithm, we need:\n",
    "* a function approximation (neural network) to calculate the _policy_ from the observation,\n",
    "* to sample episodes and calculate the returns (or a similar measure),\n",
    "* calculate the gradient of the return (involves calculating the gradients of the policy)\n",
    "* apply gradient ascent to change the weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fbd63-342b-4c84-9927-55ee5f34b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jdc\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f38177-2e5f-4ab2-a8fe-3498eb2531ae",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Our agent will again use a function to generate the neural network (the model), so that we can call it using different models. For torch we will define this as class that encapsulates the network for the policy (so derived from `nn.Module` and uses a method act to calculate the action which can then be used directly in the agent.\n",
    "\n",
    "For the loss calculation later, we will need not only the selected action, but also the (log) probability of the selected action and the gradient on it. We should make it possible to save this from the policy network. Implement the `act` method so that it return both the selected action and the log of the probability of choosing this action. You can use the method `log_prob` from `Categorical`. For the log_prob, return the tensor from torch instead of the value (i.e. do not use `item`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32154481-876b-409b-8710-ac48ea5e31e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1942b77b26f2cf4528eaa6b1c1bc5be",
     "grade": false,
     "grade_id": "cell-9022b0c42223a457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # define a function self.fc that contains the network using nn.Sequential\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # calculate the action and return it\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15105d-3f30-4196-a294-b9a18bfafcf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5182edeed8596469e4aa2b2297d911a8",
     "grade": true,
     "grade_id": "cell-b5a410bcf61bdea9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "obs_sample = env.observation_space.sample()\n",
    "action, log_prob = policy.act(obs_sample)\n",
    "print(action, log_prob)\n",
    "action_prob = policy.forward(torch.from_numpy(obs_sample).float().unsqueeze(0))\n",
    "print(action_prob)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0e04f-1a0c-4945-8271-a6a9b3cd841d",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. We will start with the class definition and the `__init__` method. Check the parameters and the descriptions as they will be used in the implementation. There is one additional array `log_prob` to save the log probabilities of the actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40388d-9b40-432e-8b76-808ecdfc2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGAgent:\n",
    "    \"\"\"\n",
    "    Implementation of (vanilla) policy gradient agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 gamma: float = 0.99,\n",
    "                 learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize agent\n",
    "        Args:\n",
    "            observation_space: the observation space of the environment\n",
    "            action_space: the action space of the environment\n",
    "            gamma: the discount factor\n",
    "            learning_rate: the learning rate\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # generate the model\n",
    "        self.policy_network = PolicyNetwork(observation_space, action_space)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # arrays to store an episode for training\n",
    "        self.obs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b4c91-576c-4689-b1d0-34d9bf306abc",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "The model directly calculates the policy, so we just have to draw an action from the resuling probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633df8f1-0628-4c9a-8c2d-fbcb5c0e7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def calculate_action(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action to take\n",
    "    Args:\n",
    "        obs: the observation\n",
    "    Returns:\n",
    "        the action to take, the log probability of the action\n",
    "    \"\"\"\n",
    "    return self.policy_network.act(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bad45-29b4-4b5e-8916-36b5b41b2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sample = env.observation_space.sample()\n",
    "agent = VPGAgent(observation_space=env.observation_space, \n",
    "                          action_space=env.action_space)\n",
    "\n",
    "action, log_prob = agent.calculate_action(obs_sample)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851fd08-2204-4df5-8dbd-c3f0fdeca5df",
   "metadata": {},
   "source": [
    "## Step functions and training\n",
    "\n",
    "Next we will add the step functions and the _training_ inside them. \n",
    "\n",
    "### step first\n",
    "\n",
    "The step first function just calculates and action appends the information for our episode (observation, action). The policy is stochastic, so we should draw from the random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd95f4-4cc3-47d2-a2fa-a39d0a3952aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    self.obs.append(obs)\n",
    "    action, log_prob = self.calculate_action(obs)\n",
    "    self.actions.append(action)\n",
    "    self.log_probs.append(log_prob)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b7763-dafc-4514-949a-8fd155378474",
   "metadata": {},
   "source": [
    "### Step and training\n",
    "\n",
    "\n",
    "Simular to MC methods, updates only occur at the end of episodes and we only use the calculated return once for the gradient, however we do this for each of the actions during this episode. Therefor the update batch has the length of an episode.\n",
    "\n",
    "The update of the gradients is according to\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta J(\\pi_\\theta)\n",
    "$$\n",
    "\n",
    "where $J(\\pi_\\theta)$ is the loss function. In general the gradient of the loss functionhas the form\n",
    "\n",
    "$$\n",
    "\\nabla\\theta J(\\pi\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^T \\nabla_\\theta\\Phi_t\\log\\pi_\\theta(a_t| s_t) \\right]\n",
    "$$\n",
    "\n",
    "where there are different choices for $\\Phi_t$. We can for example use\n",
    "\n",
    "$$\n",
    "\\Phi_t = G\n",
    "$$\n",
    "where $G$ is the (total) return of the episode, or use the obtained return from each state, sometimes also called the sum of the discounted future rewards.\n",
    "$$\n",
    "\\Phi_t = \\sum_{t'=t}^T R_t\n",
    "$$\n",
    "It can be proven, that all these choices actually lead to the same expectation of the gradient. I would suggest to use the sum of discounted future rewards.\n",
    "\n",
    "It can also help to normalize the values to zero mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddd40f-a0c6-4dec-a4da-653c2b3e2111",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f6e7533fdf7da3ec7a52ee3be12f327",
     "grade": false,
     "grade_id": "cell-254041610abfba8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    # simular to MC learning, we only update at the end of an episode\n",
    "\n",
    "    # udpate the reward from the last time step (so that all arrays should now have the same length)\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "    if not done:\n",
    "        # we have to do the same as in the first_step: add the observation and calculate and store an action\n",
    "        return self.step_first(obs)\n",
    "\n",
    "    else:\n",
    "        # an episode is finished, so we calculate the gradient and update the weights\n",
    "        assert len(self.obs) == len(self.actions)\n",
    "        assert len(self.obs) == len(self.rewards)\n",
    "        assert len(self.obs) == len(self.log_probs)\n",
    "\n",
    "        future_rewards = np.zeros_like(self.rewards)\n",
    "\n",
    "       \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        del self.rewards[:]\n",
    "        del self.obs[:]\n",
    "        del self.actions[:]\n",
    "        del self.log_probs[:]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de5afb-f3f7-459a-be76-79d8dbd329c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b1e621871255da246a96be347714680",
     "grade": true,
     "grade_id": "cell-e3cf687afa6e3d4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space, \n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "# Check if one complete episode runs through\n",
    "obs, _ = env.reset()\n",
    "action = agent.step_first(obs)\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9eb625-4c9f-4ac5-9f68-ea1e5d141baa",
   "metadata": {},
   "source": [
    "### Training and evaluation\n",
    "\n",
    "We add the train and evaluate methods in the agents, similar to the last exercise so that it is easier to run some tests. Nothing to code here. Note that the number of steps for training are episodes here, as we only change the weights at the end of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a94f74-276d-4fe9-b32c-c1d695b5da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "def train(self, env: gym.Env, \n",
    "          nr_episodes_train: int, \n",
    "          eval_env: gym.Env, \n",
    "          eval_frequency: int, \n",
    "          eval_nr_episodes: int,\n",
    "          eval_gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_train: the number of episodes to train\n",
    "        eval_env: the environment to use for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent (in episodes)\n",
    "        eval_nr_episodes: The number of episodes to evaluate\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.step(obs, reward, done or truncated)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, eval_nr_episodes, eval_gamma)\n",
    "            print(f'Evaluation: Episode trained {nr_episodes}, mean reward: {np.mean(rewards)}')\n",
    "        \n",
    "        if nr_episodes > nr_episodes_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a,_ = self.calculate_action(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        gamma_current = gamma\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a, _ = self.calculate_action(obs)\n",
    "            episode_reward += gamma_current * reward\n",
    "            gamma_current *= gamma\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0fd60-4646-4fcb-9867-c386c8148a0b",
   "metadata": {},
   "source": [
    "We train the agent for a number of steps to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759a721-3a76-422a-8596-c3e04f1d2e2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4c843c839e12e50e11a4cf3a76e51b4",
     "grade": true,
     "grade_id": "cell-cf2ec3e64d0c16f4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space, \n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "agent.train(env, nr_episodes_train=1000, eval_env=eval_env, eval_frequency=25, eval_nr_episodes=1, eval_gamma=1.0)\n",
    "\n",
    "# calculate return at end using evaluation\n",
    "return_eval = agent.evaluate(env=eval_env, nr_episodes=1, gamma=1.0)\n",
    "\n",
    "print(f'Evaluation: {return_eval}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b3047-4194-41d2-9fbe-36fb01ed0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for longer (1000 episodes) should obtain quite good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816060f-2106-4126-b51f-9ded36194e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _  = env.reset()\n",
    "for _ in range(200):\n",
    "    action, _ = agent.calculate_action(obs)\n",
    "    obs, _, _, _,_ = env.step(action)  # Take a random action\n",
    "    display_environment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f3f6a-a9e9-4291-8d6f-b6e29e67300a",
   "metadata": {},
   "source": [
    "Congratulations, you implemented a full policy gradient algorithm!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
